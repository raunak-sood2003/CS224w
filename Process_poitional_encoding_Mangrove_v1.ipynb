{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raunak-sood2003/CS224w/blob/main/Process_poitional_encoding_Mangrove_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x40tEdIzvkT9"
      },
      "source": [
        "###Mount the drive and set the directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ByWaKmEvMo3",
        "outputId": "ac1d9dd4-dc95-41f9-febf-cb90ffec236d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuO4rAFoviU6",
        "outputId": "7ce9af60-0806-435f-c24d-99c4f898c033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/CS224W ML on Graphs/2025/Project/Mangrove-data\n",
            "/content/drive/My Drive/CS224W ML on Graphs/2025/Project/Mangrove-data\n"
          ]
        }
      ],
      "source": [
        "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'cs231n/assignments/assignment2/'\n",
        "FOLDERNAME = 'CS224W ML on Graphs/2025/Project/Mangrove-data/'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "# Change directory to current path\n",
        "%cd /content/drive/My\\ Drive/$FOLDERNAME/\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcMyO0aXv0_c"
      },
      "source": [
        "### Download the original pkl data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42fyJTlhyN1I",
        "outputId": "0a434e9a-c468-44ad-b388-099f4a3d2192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n"
          ]
        }
      ],
      "source": [
        "# we're going to be using Pytorch, the most widely used framework for ML research right now.\n",
        "# Pytorch is already installed so all we have to worry about is that everything else we do is compatible with our version of Pytorch\n",
        "import torch\n",
        "import os\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOCdRl82yRCe",
        "outputId": "9b99e2cc-3f44-41e7-8ebe-113ce6ec0e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.9.0+cu126.html\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-cluster\n",
            "  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: torch-sparse, torch-scatter, torch-cluster\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp312-cp312-linux_x86_64.whl size=3039810 sha256=4cb25ff2cf808b8f24153ff02a35117d51be162f0ab9def58ce45f27103f3451\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/fa/21/bd1d78ce1629aec4ecc924a63b82f6949dda484b6321eac6f2\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp312-cp312-linux_x86_64.whl size=3857026 sha256=6d5a554c5a4133d48c7f62c2da8ca156ed1f28634d78096fdbfc6795019ae764\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/20/50/44800723f57cd798630e77b3ec83bc80bd26a1e3dc3a672ef5\n",
            "  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-cluster: filename=torch_cluster-1.6.3-cp312-cp312-linux_x86_64.whl size=2205946 sha256=befae41161179d3863c561dbe784fe4f4306d6e64c45e8efbf9546c08955b001\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/8f/d0/13408a84825c9a587151a74727b4a6d47ec67e0d625b385ad7\n",
            "Successfully built torch-sparse torch-scatter torch-cluster\n",
            "Installing collected packages: torch-scatter, torch-sparse, torch-cluster, torch-geometric\n",
            "Successfully installed torch-cluster-1.6.3 torch-geometric-2.7.0 torch-scatter-2.1.2 torch-sparse-0.6.18\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "#here we load the libraries needed for doing ML on graphs, pytorch geometric, and a little package that can help with making our training faster\n",
        "!pip install torch-geometric \\\n",
        "  torch-sparse \\\n",
        "  torch-scatter \\\n",
        "  torch-cluster \\\n",
        "  -f https://pytorch-geometric.com/whl/torch-${TORCH}.html\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGVhg1WQwrwM"
      },
      "outputs": [],
      "source": [
        "import pickle, time, os\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "from accelerate import Accelerator\n",
        "from datetime import date\n",
        "\n",
        "import torch_geometric as tg\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib as mpl\n",
        "\n",
        "font = {'family' : 'DejaVu Sans',\n",
        "        'weight' : 'normal',\n",
        "        'size'   : 16}\n",
        "\n",
        "mpl.rc('font', **font)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8ojTeVLx2PO"
      },
      "source": [
        "Only need to do this step once, otherwise it's in the '/content/drive/My Drive/CS224W ML on Graphs/2025/Project/Mangrove-data' directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs6iVdASviMS",
        "outputId": "69477720-869c-48e2-a409-f5e305c5e4ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/u/0/uc?id=192_gOXbEAhp4BSARobCsmJP2f98LsbGG\n",
            "From (redirected): https://drive.google.com/uc?id=192_gOXbEAhp4BSARobCsmJP2f98LsbGG&confirm=t&uuid=00b6047e-c797-41a3-bf40-781573be377f\n",
            "To: /content/drive/My Drive/CS224W ML on Graphs/2025/Project/Mangrove-data/data25.pkl\n",
            " 47% 790M/1.68G [00:12<00:10, 86.9MB/s]Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/gdown\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gdown/__main__.py\", line 172, in main\n",
            "    download(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gdown/download.py\", line 369, in download\n",
            "    f.write(chunk)\n",
            "KeyboardInterrupt\n",
            " 47% 791M/1.68G [00:12<00:13, 63.4MB/s]\n",
            "^C\n",
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1FjyePBL8FNfTudRDMffHmamnBVq7EKAW\n",
            "To: /content/drive/MyDrive/CS224W ML on Graphs/2025/Project/Mangrove-data/construct_dict.pkl\n",
            "100% 852/852 [00:00<00:00, 3.79MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=17U1qqOvLj2J8TK0LTFUeMJcR26xLq5bQ\n",
            "To: /content/drive/MyDrive/CS224W ML on Graphs/2025/Project/Mangrove-data/model_best.pt\n",
            "100% 2.42M/2.42M [00:00<00:00, 164MB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/gdown\", line 4, in <module>\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gdown/__init__.py\", line 6, in <module>\n",
            "    from .cached_download import cached_download\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gdown/cached_download.py\", line 12, in <module>\n",
            "    from .download import download\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gdown/download.py\", line 13, in <module>\n",
            "    import bs4\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/bs4/__init__.py\", line 65, in <module>\n",
            "    from .builder import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/bs4/builder/__init__.py\", line 832, in <module>\n",
            "    from . import _htmlparser # noqa: E402\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/bs4/builder/_htmlparser.py\", line 12, in <module>\n",
            "    from html.parser import HTMLParser\n",
            "  File \"/usr/lib/python3.12/html/parser.py\", line 12, in <module>\n",
            "    import _markupbase\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1322, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 1262, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1532, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1506, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1597, in find_spec\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!gdown 'https://drive.google.com/u/0/uc?id=192_gOXbEAhp4BSARobCsmJP2f98LsbGG' # 25% of full data file, 1.56 GB\n",
        "!gdown 'https://drive.google.com/u/0/uc?id=1FjyePBL8FNfTudRDMffHmamnBVq7EKAW' # configuration file\n",
        "!gdown 'https://drive.google.com/u/0/uc?id=17U1qqOvLj2J8TK0LTFUeMJcR26xLq5bQ' # model\n",
        "!gdown 'https://drive.google.com/u/0/uc?id=12MCG8u6TYNSD-Y5exWe1yvu7rYUHQCyR' # results file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9gbKLl0wFHR"
      },
      "outputs": [],
      "source": [
        "data = pickle.load(open('data25.pkl', 'rb'))\n",
        "config = pickle.load(open('construct_dict.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Model"
      ],
      "metadata": {
        "id": "0ouIfP4TB-Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\")\n",
        "print(\"number of graphs: \", len(data))\n",
        "print('==============================================================')\n",
        "idx = 4\n",
        "one_graph = data[idx]  # Get the first graph object.\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(\"graph at index: \", idx)\n",
        "print(one_graph)\n",
        "print(f'Number of nodes: {one_graph.num_nodes}')\n",
        "print(f'Number of edges: {one_graph.num_edges}')\n",
        "print(f'Average node degree: {(one_graph.num_edges) / one_graph.num_nodes:.2f}')\n",
        "print(f'Contains isolated nodes: {one_graph.has_isolated_nodes()}')\n",
        "print(f'Contains self-loops: {one_graph.has_self_loops()}')\n",
        "print(f'Is undirected: {one_graph.is_undirected()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25xRFiQ8BS5z",
        "outputId": "096db2f9-5a0f-44fe-8ab2-5335e1d15431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "number of graphs:  27085\n",
            "==============================================================\n",
            "graph at index:  4\n",
            "Data(x=[10230, 43], edge_index=[2, 10229], edge_attr=[10229], y=[41])\n",
            "Number of nodes: 10230\n",
            "Number of edges: 10229\n",
            "Average node degree: 1.00\n",
            "Contains isolated nodes: False\n",
            "Contains self-loops: False\n",
            "Is undirected: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute the Laplacian\n",
        "You can choose which Laplacian you want:\n",
        "\n",
        "- Combinatorial: $L=D−A$\n",
        "- Symmetric normalized: $L_{sym} =I−D^{−1/2} A D^{−1/2}$\n",
        "- Random-walk normalized: $L_{rw} = I − D^{−1} A$"
      ],
      "metadata": {
        "id": "w7Z961HZCl7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import get_laplacian, to_dense_adj, to_scipy_sparse_matrix, to_undirected\n",
        "from scipy.sparse.linalg import eigsh  # sparse symmetric eigensolver\n",
        "\n",
        "def combinatorial_laplacian(data):\n",
        "    # data.edge_index is shape [2, num_edges]\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # Returns edges and weights defining L = D - A\n",
        "    lap_edge_index, lap_edge_weight = get_laplacian(edge_index, normalization=None)\n",
        "\n",
        "    # Convert sparse representation to dense matrix\n",
        "    L = to_dense_adj(lap_edge_index, edge_attr=lap_edge_weight,\n",
        "                     max_num_nodes=data.num_nodes).squeeze(0)\n",
        "    return L\n",
        "\n",
        "\n",
        "def normalized_laplacian(data):\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # Returns edges/weights for L_sym = I - D^{-1/2} A D^{-1/2}\n",
        "    lap_edge_index, lap_edge_weight = get_laplacian(edge_index, normalization='sym')\n",
        "\n",
        "    L = to_dense_adj(lap_edge_index, edge_attr=lap_edge_weight,\n",
        "                     max_num_nodes=data.num_nodes).squeeze(0)\n",
        "    return L\n",
        "\n",
        "\n",
        "def random_walk_laplacian(data):\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # L_rw = I - D^{-1} A\n",
        "    lap_edge_index, lap_edge_weight = get_laplacian(edge_index, normalization='rw')\n",
        "\n",
        "    L = to_dense_adj(lap_edge_index, edge_attr=lap_edge_weight,\n",
        "                     max_num_nodes=data.num_nodes).squeeze(0)\n",
        "    return L\n",
        "\n",
        "\n",
        "def laplacian_positional_encoding(data, k, normalization='sym'):\n",
        "    \"\"\"\n",
        "    Compute top-k non-trivial Laplacian eigenvectors for a PyG Data graph.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data          : torch_geometric.data.Data Graph\n",
        "                    with data.edge_index and data.num_nodes.\n",
        "    k             : int Number of *non-trivial* eigenvectors\n",
        "                    to return (skips the constant one).\n",
        "    normalization : {'sym', 'rw', None}, optional\n",
        "          - 'sym' : symmetric normalized Laplacian  L = I - D^{-1/2} A D^{-1/2}\n",
        "          - 'rw'  : random-walk Laplacian           L = I - D^{-1} A\n",
        "          - None  : combinatorial Laplacian         L = D - A\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    eigvecs : torch.Tensor, shape [num_nodes, k]\n",
        "              The k Laplacian eigenvectors (columns), as float32.\n",
        "    eigvals : torch.Tensor, shape [k]\n",
        "              Corresponding eigenvalues (ascending, skipping the trivial 0).\n",
        "    \"\"\"\n",
        "    num_nodes = data.num_nodes\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # 1) Build Laplacian in sparse form\n",
        "    lap_edge_index, lap_edge_weight = get_laplacian(\n",
        "        edge_index,\n",
        "        normalization=normalization,\n",
        "        num_nodes=num_nodes\n",
        "    )\n",
        "\n",
        "    L = to_scipy_sparse_matrix(lap_edge_index, lap_edge_weight, num_nodes=num_nodes)\n",
        "\n",
        "    # 2) Compute smallest (k+1) eigenpairs: include trivial 0, then skip it\n",
        "    #    (for connected graphs; if graph is disconnected you may want more)\n",
        "    evals, evecs = eigsh(L, k=k+1, which='SM')  # SM = smallest magnitude\n",
        "\n",
        "    # 3) Sort just in case and drop the first (trivial) eigenvector\n",
        "    idx = evals.argsort()\n",
        "    evals, evecs = evals[idx], evecs[:, idx]\n",
        "\n",
        "    # drop the first eigenpair (corresponding to eigenvalue ~0)\n",
        "    evals_nontriv = evals[1:k+1]\n",
        "    evecs_nontriv = evecs[:, 1:k+1]\n",
        "\n",
        "    eigvecs = torch.from_numpy(evecs_nontriv).float()\n",
        "    eigvals = torch.from_numpy(evals_nontriv).float()\n",
        "    return eigvecs, eigvals\n",
        "\n",
        "\n",
        "def make_undirected(data):\n",
        "    data = data.clone()\n",
        "    data.edge_index = to_undirected(data.edge_index, num_nodes=data.num_nodes)\n",
        "    return data"
      ],
      "metadata": {
        "id": "0mMwBfiYBS2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L = combinatorial_laplacian(one_graph)\n",
        "print(L.shape)\n",
        "Lambda = laplacian_positional_encoding(one_graph, 10, normalization=None)\n",
        "print(Lambda[0].shape)\n",
        "print(Lambda[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikOHvVQWBSlf",
        "outputId": "7f7bbf05-28f3-4881-b392-262998f9de12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10230, 10230])\n",
            "torch.Size([10230, 10])\n",
            "tensor([0.0141, 0.0166, 0.0182, 0.0183, 0.0212, 0.0246, 0.0272, 0.0562, 0.0599,\n",
            "        0.0641])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Confirm the computed eigenvectors satisfy the eigenvector equation. Note we have to convert the graphs to undirected to get a symmetric Laplacian."
      ],
      "metadata": {
        "id": "q3hNrNykSUaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "undirected_graph = make_undirected(one_graph)\n",
        "\n",
        "L = combinatorial_laplacian(undirected_graph)\n",
        "eigvecs, eigvals = laplacian_positional_encoding(undirected_graph, 10, normalization=None)\n",
        "\n",
        "i = 2\n",
        "v = eigvecs[:, i]\n",
        "lam = eigvals[i]\n",
        "\n",
        "Lv = L @ v\n",
        "lamv = lam * v\n",
        "\n",
        "residual = Lv - lamv\n",
        "err = residual.norm().item()\n",
        "rel_err = err / (lamv.norm().item() + 1e-12)\n",
        "\n",
        "print(\"‖Lv - λv‖ =\", err)\n",
        "print(\"relative error =\", rel_err)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNcklc9dBShT",
        "outputId": "2be4d911-b461-4424-cd0c-7cdbb29c5c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‖Lv - λv‖ = 3.320720134070143e-05\n",
            "relative error = 0.5093721702625378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute a new dataset with augmented positional information"
      ],
      "metadata": {
        "id": "Y3zaDmq8TFfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data, Dataset\n",
        "\n",
        "def augment_with_lappe(data: Data, k: int, normalization=None) -> Data:\n",
        "    \"\"\"\n",
        "    Return a clone of `data` where node features are augmented with\n",
        "    k-dimensional Laplacian positional encodings.\n",
        "    \"\"\"\n",
        "    # Work on an undirected version for symmetric Laplacian\n",
        "    data_u = make_undirected(data)\n",
        "\n",
        "    # Compute eigenvectors on the undirected graph\n",
        "    eigvecs, eigvals = laplacian_positional_encoding(\n",
        "        data_u, k=k, normalization=normalization\n",
        "    )  # eigvecs: [N, k_eff]\n",
        "\n",
        "    # If we requested k but got fewer (small graph), we can pad or just concat as-is.\n",
        "    # Here we just concat as-is; dimensions will be <= k.\n",
        "    pe = eigvecs  # [num_nodes, k_eff]\n",
        "\n",
        "    aug_data = data.clone()\n",
        "\n",
        "    if aug_data.x is None:\n",
        "        aug_data.x = pe\n",
        "    else:\n",
        "        # Concatenate along feature dimension\n",
        "        aug_data.x = torch.cat([aug_data.x, pe], dim=-1)\n",
        "\n",
        "    # Optionally store eigenvalues too if you want them later:\n",
        "    aug_data.lap_eigvals = eigvals\n",
        "\n",
        "    return aug_data\n",
        "\n",
        "\n",
        "def augment_dataset_with_lappe(dataset: Dataset, k: int, normalization=None):\n",
        "    \"\"\"\n",
        "    Iterate through a PyG Dataset, augment each graph with Laplacian PEs,\n",
        "    and return a list of augmented Data objects.\n",
        "    \"\"\"\n",
        "    augmented_graphs = []\n",
        "\n",
        "    for data in tqdm(dataset, desc=\"Augmenting graphs with Laplacian PEs\"):\n",
        "        if data.num_nodes > 5000 or data.num_nodes <= 2*k:\n",
        "            continue\n",
        "        aug_data = augment_with_lappe(data, k=k, normalization=normalization)\n",
        "        augmented_graphs.append(aug_data)\n",
        "\n",
        "    return augmented_graphs\n",
        "    '''\n",
        "    for i, data in enumerate(dataset):\n",
        "        if data.num_nodes > 5000:\n",
        "            continue\n",
        "        print(f\"Processing graph {i+1}/{len(dataset)} ...\")\n",
        "        aug_data = augment_with_lappe(data, k=k, normalization=normalization)\n",
        "        augmented_graphs.append(aug_data)\n",
        "    return augmented_graphs\n",
        "    '''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "la0HUt1TFA-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data, Dataset\n",
        "from torch_geometric.utils import get_laplacian, to_dense_adj, to_scipy_sparse_matrix, to_undirected\n",
        "from scipy.sparse.linalg import eigsh  # sparse symmetric eigensolver\n",
        "\n",
        "def make_undirected(data):\n",
        "    data = data.clone()\n",
        "    data.edge_index = to_undirected(data.edge_index, num_nodes=data.num_nodes)\n",
        "    return data\n",
        "\n",
        "def laplacian_positional_encoding(data, k, normalization='sym'):\n",
        "    \"\"\"\n",
        "    Compute top-k non-trivial Laplacian eigenvectors for a PyG Data graph.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data          : torch_geometric.data.Data Graph\n",
        "                    with data.edge_index and data.num_nodes.\n",
        "    k             : int Number of *non-trivial* eigenvectors\n",
        "                    to return (skips the constant one).\n",
        "    normalization : {'sym', 'rw', None}, optional\n",
        "          - 'sym' : symmetric normalized Laplacian  L = I - D^{-1/2} A D^{-1/2}\n",
        "          - 'rw'  : random-walk Laplacian           L = I - D^{-1} A\n",
        "          - None  : combinatorial Laplacian         L = D - A\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    eigvecs : torch.Tensor, shape [num_nodes, k]\n",
        "              The k Laplacian eigenvectors (columns), as float32.\n",
        "    eigvals : torch.Tensor, shape [k]\n",
        "              Corresponding eigenvalues (ascending, skipping the trivial 0).\n",
        "    \"\"\"\n",
        "    num_nodes = data.num_nodes\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # 1) Build Laplacian in sparse form\n",
        "    lap_edge_index, lap_edge_weight = get_laplacian(\n",
        "        edge_index,\n",
        "        normalization=normalization,\n",
        "        num_nodes=num_nodes\n",
        "    )\n",
        "\n",
        "    L = to_scipy_sparse_matrix(lap_edge_index, lap_edge_weight, num_nodes=num_nodes)\n",
        "\n",
        "    # 2) Compute smallest (k+1) eigenpairs: include trivial 0, then skip it\n",
        "    #    (for connected graphs; if graph is disconnected you may want more)\n",
        "    nev = k + 1\n",
        "    # Calculate an appropriate ncv value to prevent ArpackError\n",
        "    # ncv must be between nev + 1 and num_nodes - 1.\n",
        "    # A common heuristic is ncv = max(2 * nev + 1, nev + 8).\n",
        "    ncv_min_req = nev + 1\n",
        "    ncv_heuristic = max(2 * nev + 1, nev + 8)\n",
        "    ncv = min(num_nodes - 1, ncv_heuristic)\n",
        "\n",
        "    # Ensure ncv is at least the minimum required for eigsh to function\n",
        "    if ncv < ncv_min_req:\n",
        "        ncv = ncv_min_req\n",
        "\n",
        "    # Also, ensure ncv does not exceed num_nodes - 1 (or num_nodes in some cases, but num_nodes-1 is safer)\n",
        "    ncv = min(ncv, num_nodes - 1)\n",
        "\n",
        "    # If, after all adjustments, ncv is still not greater than nev, eigsh will likely fail.\n",
        "    # This typically means the graph is too small for the requested number of eigenvectors.\n",
        "    if ncv <= nev:\n",
        "        raise ValueError(f\"Cannot compute {nev} eigenvectors for graph with {num_nodes} nodes, 'k' might be too large for this graph size. Current k={k}, num_nodes={num_nodes}, ncv={ncv}.\")\n",
        "\n",
        "    evals, evecs = eigsh(L, k=nev, which='SM', ncv=ncv)  # SM = smallest magnitude\n",
        "\n",
        "    # 3) Sort just in case and drop the first (trivial) eigenvector\n",
        "    idx = evals.argsort()\n",
        "    evals, evecs = evals[idx], evecs[:, idx]\n",
        "\n",
        "    # drop the first eigenpair (corresponding to eigenvalue ~0)\n",
        "    evals_nontriv = evals[1:k+1]\n",
        "    evecs_nontriv = evecs[:, 1:k+1]\n",
        "\n",
        "    eigvecs = torch.from_numpy(evecs_nontriv).float()\n",
        "    eigvals = torch.from_numpy(evals_nontriv).float()\n",
        "    return eigvecs, eigvals\n",
        "\n",
        "def augment_with_lappe(data: Data, k: int, normalization=None) -> Data:\n",
        "    \"\"\"\n",
        "    Return a clone of `data` where node features are augmented with\n",
        "    k-dimensional Laplacian positional encodings.\n",
        "    \"\"\"\n",
        "    # Work on an undirected version for symmetric Laplacian\n",
        "    data_u = make_undirected(data)\n",
        "\n",
        "    # Compute eigenvectors on the undirected graph\n",
        "    eigvecs, eigvals = laplacian_positional_encoding(\n",
        "        data_u, k=k, normalization=normalization\n",
        "    )  # eigvecs: [N, k_eff]\n",
        "\n",
        "    # If we requested k but got fewer (small graph), we can pad or just concat as-is.\n",
        "    # Here we just concat as-is; dimensions will be <= k.\n",
        "    pe = eigvecs  # [num_nodes, k_eff]\n",
        "\n",
        "    aug_data = data.clone()\n",
        "\n",
        "    if aug_data.x is None:\n",
        "        aug_data.x = pe\n",
        "    else:\n",
        "        # Concatenate along feature dimension\n",
        "        aug_data.x = torch.cat([aug_data.x, pe], dim=-1)\n",
        "\n",
        "    # Optionally store eigenvalues too if you want them later:\n",
        "    aug_data.lap_eigvals = eigvals\n",
        "\n",
        "    return aug_data\n",
        "\n",
        "def augment_dataset_with_lappe(dataset: Dataset, k: int, normalization=None):\n",
        "    \"\"\"\n",
        "    Iterate through a PyG Dataset, augment each graph with Laplacian PEs,\n",
        "    and return a list of augmented Data objects.\n",
        "    \"\"\"\n",
        "    augmented_graphs = []\n",
        "\n",
        "    for data in tqdm(dataset, desc=\"Augmenting graphs with Laplacian PEs\"):\n",
        "        if data.num_nodes > 5000 or data.num_nodes <= 3*k:\n",
        "            continue\n",
        "        aug_data = augment_with_lappe(data, k=k, normalization=normalization)\n",
        "        augmented_graphs.append(aug_data)\n",
        "\n",
        "    return augmented_graphs\n"
      ],
      "metadata": {
        "id": "_YIVaGJhTUlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9c7773a",
        "outputId": "d03349ed-d826-4d38-e633-a0e0a0b66fe3"
      },
      "source": [
        "import pickle\n",
        "\n",
        "ks = [7, 9]  # You can adjust this value for the number of Laplacian PE dimensions\n",
        "normalization = None  # Choose 'sym', 'rw', or None for combinatorial Laplacian\n",
        "\n",
        "for k in ks:\n",
        "  # Call augment_dataset_with_lappe to process the dataset\n",
        "  augmented_graphs = augment_dataset_with_lappe(data, k=k, normalization=normalization)\n",
        "\n",
        "  # Define the output path for the pickle file\n",
        "  output_path = f\"dataset_with_lapPE{k}.pkl\"\n",
        "\n",
        "  # Save the augmented dataset as a pickle file\n",
        "  with open(output_path, \"wb\") as f:\n",
        "      pickle.dump(augmented_graphs, f)\n",
        "\n",
        "  print(f\"Saved augmented dataset with Laplacian PEs to: {output_path}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Augmenting graphs with Laplacian PEs: 100%|██████████| 27085/27085 [34:42<00:00, 13.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved augmented dataset with Laplacian PEs to: dataset_with_lapPE7.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Augmenting graphs with Laplacian PEs: 100%|██████████| 27085/27085 [36:06<00:00, 12.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved augmented dataset with Laplacian PEs to: dataset_with_lapPE9.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25627a13"
      },
      "source": [
        "from torch_geometric.utils import get_laplacian, to_dense_adj, to_scipy_sparse_matrix, to_undirected\n",
        "from scipy.sparse.linalg import eigsh  # sparse symmetric eigensolver\n",
        "\n",
        "def laplacian_positional_encoding(data, k, normalization='sym'):\n",
        "    \"\"\"\n",
        "    Compute top-k non-trivial Laplacian eigenvectors for a PyG Data graph.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data          : torch_geometric.data.Data Graph\n",
        "                    with data.edge_index and data.num_nodes.\n",
        "    k             : int Number of *non-trivial* eigenvectors\n",
        "                    to return (skips the constant one).\n",
        "    normalization : {'sym', 'rw', None}, optional\n",
        "          - 'sym' : symmetric normalized Laplacian  L = I - D^{-1/2} A D^{-1/2}\n",
        "          - 'rw'  : random-walk Laplacian           L = I - D^{-1} A\n",
        "          - None  : combinatorial Laplacian         L = D - A\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    eigvecs : torch.Tensor, shape [num_nodes, k]\n",
        "              The k Laplacian eigenvectors (columns), as float32.\n",
        "    eigvals : torch.Tensor, shape [k]\n",
        "              Corresponding eigenvalues (ascending, skipping the trivial 0).\n",
        "    \"\"\"\n",
        "    num_nodes = data.num_nodes\n",
        "    edge_index = data.edge_index\n",
        "\n",
        "    # 1) Build Laplacian in sparse form\n",
        "    lap_edge_index, lap_edge_weight = get_laplacian(\n",
        "        edge_index,\n",
        "        normalization=normalization,\n",
        "        num_nodes=num_nodes\n",
        "    )\n",
        "\n",
        "    L = to_scipy_sparse_matrix(lap_edge_index, lap_edge_weight, num_nodes=num_nodes)\n",
        "\n",
        "    # 2) Compute smallest (k+1) eigenpairs: include trivial 0, then skip it\n",
        "    #    (for connected graphs; if graph is disconnected you may want more)\n",
        "    nev = k + 1\n",
        "    # Calculate an appropriate ncv value to prevent ArpackError\n",
        "    # ncv must be between nev + 1 and num_nodes - 1.\n",
        "    # A common heuristic is ncv = max(2 * nev + 1, nev + 8).\n",
        "    ncv_min_req = nev + 1\n",
        "    ncv_heuristic = max(2 * nev + 1, nev + 8)\n",
        "    ncv = min(num_nodes - 1, ncv_heuristic)\n",
        "\n",
        "    # Ensure ncv is at least the minimum required for eigsh to function\n",
        "    if ncv < ncv_min_req:\n",
        "        ncv = ncv_min_req\n",
        "\n",
        "    # Also, ensure ncv does not exceed num_nodes - 1 (or num_nodes in some cases, but num_nodes-1 is safer)\n",
        "    ncv = min(ncv, num_nodes - 1)\n",
        "\n",
        "    # If, after all adjustments, ncv is still not greater than nev, eigsh will likely fail.\n",
        "    # This typically means the graph is too small for the requested number of eigenvectors.\n",
        "    if ncv <= nev:\n",
        "        raise ValueError(f\"Cannot compute {nev} eigenvectors for graph with {num_nodes} nodes, 'k' might be too large for this graph size. Current k={k}, num_nodes={num_nodes}, ncv={ncv}.\")\n",
        "\n",
        "    evals, evecs = eigsh(L, k=nev, which='SM', ncv=ncv)  # SM = smallest magnitude\n",
        "\n",
        "    # 3) Sort just in case and drop the first (trivial) eigenvector\n",
        "    idx = evals.argsort()\n",
        "    evals, evecs = evals[idx], evecs[:, idx]\n",
        "\n",
        "    # drop the first eigenpair (corresponding to eigenvalue ~0)\n",
        "    evals_nontriv = evals[1:k+1]\n",
        "    evecs_nontriv = evecs[:, 1:k+1]\n",
        "\n",
        "    eigvecs = torch.from_numpy(evecs_nontriv).float()\n",
        "    eigvals = torch.from_numpy(evals_nontriv).float()\n",
        "    return eigvecs, eigvals\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_data = augment_with_lappe(data[6], k=8, normalization=None)\n",
        "print(aug_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "p1Jfls9CFA7t",
        "outputId": "5d13a0e0-c611-4448-e987-0b8f584dfd5e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1020982980.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maug_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_with_lappe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2459438863.py\u001b[0m in \u001b[0;36maugment_with_lappe\u001b[0;34m(data, k, normalization)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Compute eigenvectors on the undirected graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     eigvecs, eigvals = laplacian_positional_encoding(\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mdata_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     )  # eigvecs: [N, k_eff]\n",
            "\u001b[0;32m/tmp/ipython-input-3498805705.py\u001b[0m in \u001b[0;36mlaplacian_positional_encoding\u001b[0;34m(data, k, normalization)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# 2) Compute smallest (k+1) eigenpairs: include trivial 0, then skip it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m#    (for connected graphs; if graph is disconnected you may want more)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigsh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SM'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# SM = smallest magnitude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# 3) Sort just in case and drop the first (trivial) eigenvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py\u001b[0m in \u001b[0;36meigsh\u001b[0;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_ARPACK_LOCK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m             \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_eigenvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py\u001b[0m in \u001b[0;36miterate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mido\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipntr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m             self._arpack_solver(self.ido, self.bmat, self.which, self.k,\n\u001b[0m\u001b[1;32m    546\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miparam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                                 self.ipntr, self.workd, self.workl, self.info)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\")\n",
        "print(\"number of graphs: \", len(augmented_graphs))\n",
        "print('==============================================================')\n",
        "idx = 4\n",
        "one_graph = augmented_graphs[idx]  # Get the first graph object.\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(\"graph at index: \", idx)\n",
        "print(one_graph)\n",
        "print(f'Number of nodes: {one_graph.num_nodes}')\n",
        "print(f'Number of edges: {one_graph.num_edges}')\n",
        "print(f'Average node degree: {(one_graph.num_edges) / one_graph.num_nodes:.2f}')\n",
        "print(f'Contains isolated nodes: {one_graph.has_isolated_nodes()}')\n",
        "print(f'Contains self-loops: {one_graph.has_self_loops()}')\n",
        "print(f'Is undirected: {one_graph.is_undirected()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKO8i9PrFA4m",
        "outputId": "7d4d77a1-34ac-4231-db24-711156c3dbb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "number of graphs:  26877\n",
            "==============================================================\n",
            "graph at index:  4\n",
            "Data(x=[580, 51], edge_index=[2, 579], edge_attr=[579], y=[41], lap_eigvals=[8])\n",
            "Number of nodes: 580\n",
            "Number of edges: 579\n",
            "Average node degree: 1.00\n",
            "Contains isolated nodes: False\n",
            "Contains self-loops: False\n",
            "Is undirected: False\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN7L9AE+ICnh56g53PSslIY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}